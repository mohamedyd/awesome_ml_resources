| Chapter | Topic     | Notes      |
|---------|-----------|------------|
| 1       | What are Large Language Models?   |  * Large language models (LLMs) are deep neural network models for Natural Language Processing (NLP).<br>  * when we say language models "understand," we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.<br> * **Distinctions** between LLMs and earlier NLP models: <ul><li> LLMs are trained on vast quantities of text data. This allows LLMs to capture deeper contextual information and subtleties of human language.</li><li> NLP models typically designed for specific tasks, while LMs demonstrate a broader proficiency across a wide range of NLP tasks, e.g.,  ext translation, sentiment analysis, question answering.</li></ul> *  The success behind LLMs can be attributed to the <a href="https://arxiv.org/abs/1706.03762">transformer architecture</a> underpins many LLMs, and the **vast amounts of data** LLMs are trained on.<br> * The "large" in large language model refers to both the model's size in terms of parameters and the immense dataset on which it's trained.<br> *  LLM models often have tens or even hundreds of billions of parameters, the adjustable weights in the network optimized during training to **predict the next word** in a sequence.<br> * Existing LLMs can be adapted and finetuned to outperform general LLMs as well, which <a href="https://arxiv.org/abs/2305.09617">teams from Google Research and Google DeepMind</a> showed in a medical context and a team at Bloomberg showed via a version of GPT pretrained on finance data from scratch <a href="https://arxiv.org/abs/2303.17564">(BloombergGPT)</a>.<br> * The process of creating an LLM includes **pretraining** and **finetuning**.<ul><li> Pretraining: LLM is trained on large, diverse **unlabeled text data** to develop a broad understanding of language. In this phase, LLMs leverage <i>self-supervised learning</i>, where the model generates its own labels from the input data.</li><li> Finetuning: pretrained LLM is then trained on a narrower **labeled dataset** more specific to particular tasks or domains. The two most common categories for finetuning LLMs are:<ul><li> **Instruction-finetuning**: the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text.</li><li> **classification finetuning**, the labeled dataset consists of texts and associated class labels, for example, emails associated with spam and non-spam labels.</li></ul></ul> |
| 1     |  Using LLMs for Different Tasks | * The transformer consists of two parts, an **encoder** that processes the input text and produces an embedding representation (a numerical representation that captures many different factors in different dimensions) of the text that the decoder can use to generate the translated text one word at a time.<br> * The encoder and decoder consist of many layers connected by a so-called **self-attention mechanism**.<ul><li> The self-attention mechanism allows the model to weigh the importance of different words or tokens in a sequence relative to each other.</li><li> This mechanism enables the model to capture longrange dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.</li></ul> * BERT (Bidirectional Encoder Representation from Transformers) and GPT (Generative Pretrained Transformers) are later varients of the transformer architecture.<br> * BERT and its variants specialize in masked word prediction, which makes them suitable for text classification tasks (e.g., document classification and sentiment prediction).<br> * GPT models rely on the decoder submodule to perform text completion tasks, e.g., text summarization and code generation. GPT models work with eithr zero-shot learning or few-shot learning.<ul><li> Zero-shot learning: the ability to generalize to completely unseen tasks without any prior specific examples.</li><li> Few-shot learning: learning from a minimal number of examples the user provides as input.</li></ul> * Not all GPT models are transformers (some GPTs reply on recurrent convolutional networks, to improve computational efficiency) and not all transformers are GPT models (since transformers can also be used for computer vision).<br> * Pretraining LLMs requires access to significant resources and is very expensive. For example, GPT-3 pretraining cost is estimated to be $4.6 million in terms of cloud computing credits.<br> |
|  1   | A closer look at the GPT architecture  | * ChatGPT model was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI's InstructGPT paper: <a href="http://cdn.openai.com/researchcovers/language-unsupervised/language_understanding_paper.pdf">Finetuning with Human Feedback To Follow Instructions</a>.<br> * Self-supervised learning uses the next word in a sentence as the label that the model is supposed to predict.<br> * Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of **autoregressive** model.<br> * GPT-3 is significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total. |
| Appendix A  |  Introduction to PyTorch  | * <a href="https://pytorch. org/">Pytorch</a> is an open-source Python-based deep learning library. Despite its accessibility, it does not compromise on flexibility, providing advanced users the ability to tweak lower-level aspects of their models for customization and optimization.<br> * PyTorch's three main components include:<ul><li> A tensor library as a fundamental building block for efficient computing, which extends Numpy to offer a seamless switch between CPUs and GPUs; </li><li> automatic differentiation engine (aka autograd) which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization; and </li><li> deep learning library, that offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models.</li></ul> * If you do not have access to a GPU, there are several cloud computing providers where users can run GPU computations against an hourly cost. A popular Jupyter-notebook-like environment is [Google Colab](https://colab.research.google.com), [Kaggle Code](https://www.kaggle.com/code), [Paperspace Gradient](https://www.paperspace.com/gradient/free-gpu) which provides timelimited access to GPUs. |
| Appendix A | Understanding Tensors | * Tensors represent a mathematical concept that generalizes scalar values (0D), vectors (1D) and matrices (2D) to potentially higher dimensions.<br> * Tensors serve as data containers holding multi-dimensional data, where PyTorch can create, manipulate, and compute with these multidimensional arrays efficiently.<br> * PyTorch tensors are similar to <a href="https://sebastianraschka.com/blog/2020/numpy-intro.html">NumPy</a> arrays but have several additional features important for deep learning, e.g., automatic differentiation engine and GPU support.<br> * PyTorch's autograd system provides functions to compute gradients in dynamic computational graphs automatically.<br> A computation graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network.<br> * If we carry out computations in PyTorch, it will build a graph internally by default if one of its terminal nodes has the **requires_grad** attribute set to True.<br> * Computing the loss gradients in a computation graph involves applying the **chain rule** from right to left, called backpropagation. <br> * The gradients are used to update each parameter in a way that minimizes the loss function, such a method called **gradient descent**. <br> * By default, PyTorch destroys the computation graph after calculating the gradients to free memory. If we are going to reuse this computation graph, we can set **retain_graph=True** so that it stays in memory.<br> * PyTorch provides a high-level tools to automate backpropagation via calling **.backward** on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph.|
| Appendix A| Implementing multilayer neural networks| * When implementing a neural network in PyTorch, we typically subclass the **torch.nn.Module** class which allows us to encapsulate layers and operations and keep track of the model's parameters.<br> * Within this subclass, we define the network layers in the **__init\__** constructor and specify how they interact in the forward method.<br> * The outputs of the last layer of a neural network are called logits. <br> * Using **torch.nn.Sequential** can make our life easier if we have a series of layers that we want to execute in a specific order.<br><ul><li> after instantiating **self.layers = Sequential(...)** in the **__init\__** constructor, we just have to call the **self.layers** instead of calling each layer individually in the NeuralNetwork's forward method.</li></ul> * A linear layer multiplies the inputs with a weight matrix and adds a bias vector. This is sometimes referred to as a feedforward or fully connected layer.<br> * In deep learning, initializing model weights with small random numbers is desired to break symmetry during training -- otherwise, the nodes would be just performing the same operations and updates during backpropagation, which would not allow the network to learn complex mappings from inputs to outputs.<ul><li> we can make the random number initialization reproducible by seeding PyTorch's random number generator via torch.manual_seed(int).</li></ul> * When we use a model for inference (for instance, making predictions) rather than training, it is a best practice to use the **torch.no_grad()** context manager to tells PyTorch that it doesn't need to keep track of the gradients, which can result in significant savings in memory and computation.<br> * In PyTorch, it's common practice to code models such that they return the logits without passing them to a nonlinear activation function since PyTorch's commonly used loss functions combine the softmax operation with the negative log-likelihood loss in a single class for efficiency. |
| Appendix A | Data Loaders | * PyTorch implements a **Dataset** and a **DataLoader** class. The Dataset class is used to instantiate objects that define how each data record is loaded. The DataLoader handles how the data is shuffled and assembled into batches.<br> * In PyTorch, the three main components of a custom Dataset class are the __init\__ constructor, the __getitem\__ method, and the __len\__ method.<br> * Loading data without multiple workers (setting num_workers=0 ) will create a data loading bottleneck where the model sits idle until the next batch is loaded as illustrated in the left subpanel. If multiple workers are enabled, the data loader can already queue up the next batch in the background. <ul><li> if you are working with tiny datasets or interactive environments such as Jupyter notebooks, increasing num_workers may not provide any noticeable speedup. They might, in fact, lead to some issues.</li><li> One potential issue is the overhead of spinning up multiple worker processes, which could take longer than the actual data loading when your dataset is small.</li></ul> * In practice, we often use a third dataset, a so-called validation dataset, to find the optimal hyperparameter settings.<br> * model.train() and model.eval() are used to put the model into a training and an evaluation mode. This is necessary for components that behave differently during training and inference, such as dropout or batch normalization layers.<br> * It is important to include an optimizer.zero_grad() call in each update round to reset the gradients to zero. Otherwise, the gradients will accumulate, which may be undesired. |
| Appendix A | Optimizing Training with GPUs | * In PyTorch, a device is where computations occur, and data resides. The CPU and the GPU are examples of devices.<br> * If your machine hosts multiple GPUs, you have the option to specify which GPU you'd like to transfer the tensors to (you can use **.to("cuda:0")**, **.to("cuda:1")**, so on).<br> * All tensors must be on the same device. Otherwise, the computation will fail.<br> * The following will make the same code executable on a CPU if a GPU is not available: **device = torch.device("cuda" if torch.cuda.is_available() else "cpu")**.<br> * Distributed training is the concept of dividing the model training across multiple GPUs and machines.<br> * DistributedDataParallel (DDP) enables parallelism by splitting the input data across the available devices and processing these data subsets simultaneously.<ul><li> PyTorch launches a separate process on each GPU, and each process receives and keeps a copy of the model -- these copies will be synchronized during training.</li><li> the input data into unique minibatches that we pass on to each model copy.</li><li> Since each model copy will see a different sample of the training data, the model copies will return different logits as outputs and compute different gradients during the backward pass. These gradients are then averaged and synchronized during training to update the models.</li></ul> |
| 2 | Working with Text Data | * The concept of converting data into continous-valued vectors is called embedding.<ul><li> Using a specific neural network layer or another pretrained neural network model, we can embed different data types, for example, video, audio, and text.</li><li> While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents.</li><li> Sentence or paragraph embeddings are popular choices for retrieval-augmented generation.</li><li> There are several algorithms and frameworks that have been developed to generate word embeddings. One of the earlier and most popular examples is the **Word2Vec** approach.</li><li> In Word2Vec, words corresponding to similar concepts often appear close to each other in the embedding space.</li></ul> * LLMs commonly produce their own embeddings that are part of the input layer and are updated during training.<ul><li> The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand.  |
| 2  | Tokenizing data  | * Tokenization is a required preprocessing step for creating embeddings for an LLM. The tokens are either individual words or special characters including punctuation characters. <br> * When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing. <br> * We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value.<br> * We add special tokens to a vocabulary to deal with certain contexts. We add an `<\|unk\|>` token to represent new and unknown words that were not part of the training data and thus not part of the existing vocabulary. Furthermore, we add an `<\|endoftext\|>` token that we can use to separate two unrelated text sources.<br> * Starting with a new text sample, we tokenize the text and use the vocabulary to convert the text tokens into token IDs.<br> * Tokenizer implementations share two common methods: an encode method and a decode method. The encode method takes in the sample text, splits it into individual tokens, and converts the tokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back into text tokens, and concatenates the text tokens into natural text.<br> * The Byte Pair Encoding (BPE) tokenizer was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.<ul><li>  The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words.<\li><li> BPE builds its vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words.<\li><\ul> |
|  2 | Data sampling with a sliding window | * Given a text sample, extract input blocks as subsamples that serve as input to the LLM, and the LLM's prediction task during training is to predict the next word that follows the input block. During training, we mask out all words that are past the target.<br> * To implement efficient data loaders, we collect the inputs in a tensor, x, where each row represents one input context. A second tensor, y, contains the corresponding prediction targets (next words), which are created by shifting the input by one position.<br>  | 
























