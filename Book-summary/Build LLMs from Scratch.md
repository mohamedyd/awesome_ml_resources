| Chapter | Topic     | Notes      |
|---------|-----------|------------|
| 1       | Understanding Large Language Models   |  * Large language models (LLMs) are deep neural network models for Natural Language Processing (NLP).<br>  * when we say language models "understand," we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.<br> * **Distinctions** between LLMs and earlier NLP models: <ul><li> LLMs are trained on vast quantities of text data. This allows LLMs to capture deeper contextual information and subtleties of human language.</li><li> NLP models typically designed for specific tasks, while LMs demonstrate a broader proficiency across a wide range of NLP tasks, e.g.,  ext translation, sentiment analysis, question answering.</li></ul> *  The success behind LLMs can be attributed to the <a href="https://arxiv.org/abs/1706.03762">transformer architecture</a> underpins many LLMs, and the **vast amounts of data** LLMs are trained on.<br> * The "large" in large language model refers to both the model's size in terms of parameters and the immense dataset on which it's trained.<br> *  LLM models often have tens or even hundreds of billions of parameters, the adjustable weights in the network optimized during training to **predict the next word** in a sequence.<br> * Existing LLMs can be adapted and finetuned to outperform general LLMs as well, which <a href="https://arxiv.org/abs/2305.09617">teams from Google Research and Google DeepMind</a> showed in a medical context and a team at Bloomberg showed via a version of GPT pretrained on finance data from scratch <a href="https://arxiv.org/abs/2303.17564">(BloombergGPT)</a>.<br> * The process of creating an LLM includes **pretraining** and **finetuning**.<ul><li> Pretraining: LLM is trained on large, diverse **unlabeled text data** to develop a broad understanding of language. In this phase, LLMs leverage <i>self-supervised learning</i>, where the model generates its own labels from the input data.</li><li> Finetuning: pretrained LLM is then trained on a narrower **labeled dataset** more specific to particular tasks or domains. The two most common categories for finetuning LLMs are:<ul><li> **Instruction-finetuning**: the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text.</li><li> **classification finetuning**, the labeled dataset consists of texts and associated class labels, for example, emails associated with spam and non-spam labels.</li></ul></ul> * |