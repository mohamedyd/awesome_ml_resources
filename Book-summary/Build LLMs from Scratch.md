| Chapter | Topic     | Notes      |
|---------|-----------|------------|
| 1       | What are Large Language Models?   |  * Large language models (LLMs) are deep neural network models for Natural Language Processing (NLP).<br>  * when we say language models "understand," we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.<br> * **Distinctions** between LLMs and earlier NLP models: <ul><li> LLMs are trained on vast quantities of text data. This allows LLMs to capture deeper contextual information and subtleties of human language.</li><li> NLP models typically designed for specific tasks, while LMs demonstrate a broader proficiency across a wide range of NLP tasks, e.g.,  ext translation, sentiment analysis, question answering.</li></ul> *  The success behind LLMs can be attributed to the <a href="https://arxiv.org/abs/1706.03762">transformer architecture</a> underpins many LLMs, and the **vast amounts of data** LLMs are trained on.<br> * The "large" in large language model refers to both the model's size in terms of parameters and the immense dataset on which it's trained.<br> *  LLM models often have tens or even hundreds of billions of parameters, the adjustable weights in the network optimized during training to **predict the next word** in a sequence.<br> * Existing LLMs can be adapted and finetuned to outperform general LLMs as well, which <a href="https://arxiv.org/abs/2305.09617">teams from Google Research and Google DeepMind</a> showed in a medical context and a team at Bloomberg showed via a version of GPT pretrained on finance data from scratch <a href="https://arxiv.org/abs/2303.17564">(BloombergGPT)</a>.<br> * The process of creating an LLM includes **pretraining** and **finetuning**.<ul><li> Pretraining: LLM is trained on large, diverse **unlabeled text data** to develop a broad understanding of language. In this phase, LLMs leverage <i>self-supervised learning</i>, where the model generates its own labels from the input data.</li><li> Finetuning: pretrained LLM is then trained on a narrower **labeled dataset** more specific to particular tasks or domains. The two most common categories for finetuning LLMs are:<ul><li> **Instruction-finetuning**: the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text.</li><li> **classification finetuning**, the labeled dataset consists of texts and associated class labels, for example, emails associated with spam and non-spam labels.</li></ul></ul> |
| 1     |  Using LLMs for Different Tasks | * The transformer consists of two parts, an **encoder** that processes the input text and produces an embedding representation (a numerical representation that captures many different factors in different dimensions) of the text that the decoder can use to generate the translated text one word at a time.<br> * The encoder and decoder consist of many layers connected by a so-called **self-attention mechanism**.<ul><li> The self-attention mechanism allows the model to weigh the importance of different words or tokens in a sequence relative to each other.</li><li> This mechanism enables the model to capture longrange dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.</li></ul> * BERT (Bidirectional Encoder Representation from Transformers) and GPT (Generative Pretrained Transformers) are later varients of the transformer architecture.<br> * BERT and its variants specialize in masked word prediction, which makes them suitable for text classification tasks (e.g., document classification and sentiment prediction).<br> * GPT models rely on the decoder submodule to perform text completion tasks, e.g., text summarization and code generation. GPT models work with eithr zero-shot learning or few-shot learning.<ul><li> Zero-shot learning: the ability to generalize to completely unseen tasks without any prior specific examples.</li><li> Few-shot learning: learning from a minimal number of examples the user provides as input.</li></ul> * Not all GPT models are transformers (some GPTs reply on recurrent convolutional networks, to improve computational efficiency) and not all transformers are GPT models (since transformers can also be used for computer vision).<br> * Pretraining LLMs requires access to significant resources and is very expensive. For example, GPT-3 pretraining cost is estimated to be $4.6 million in terms of cloud computing credits.<br> |
|  1   | A closer look at the GPT architecture  | * ChatGPT model was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI's InstructGPT paper: <a href="http://cdn.openai.com/researchcovers/language-unsupervised/language_understanding_paper.pdf">Finetuning with Human Feedback To Follow Instructions</a>.<br> * Self-supervised learning uses the next word in a sentence as the label that the model is supposed to predict.<br> * Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of **autoregressive** model.<br> * GPT-3 is significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total. |
| Appendix A  |  Introduction to PyTorch  | * <a href="https://pytorch. org/">Pytorch</a> is an open-source Python-based deep learning library. Despite its accessibility, it does not compromise on flexibility, providing advanced users the ability to tweak lower-level aspects of their models for customization and optimization.<br> * PyTorch's three main components include:<ul><li> A tensor library as a fundamental building block for efficient computing, which extends Numpy to offer a seamless switch between CPUs and GPUs; </li><li> automatic differentiation engine (aka autograd) which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization; and </li><li> deep learning library, that offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models.</li></ul> * If you do not have access to a GPU, there are several cloud computing providers where users can run GPU computations against an hourly cost. A popular Jupyter-notebook-like environment is [Google Colab](https://colab.research.google.com), [Kaggle Code](https://www.kaggle.com/code), [Paperspace Gradient](https://www.paperspace.com/gradient/free-gpu) which provides timelimited access to GPUs. |
| Appendix A | Understanding Tensors | * Tensors represent a mathematical concept that generalizes scalar values (0D), vectors (1D) and matrices (2D) to potentially higher dimensions.<br> * Tensors serve as data containers holding multi-dimensional data, where PyTorch can create, manipulate, and compute with these multidimensional arrays efficiently.<br> * PyTorch tensors are similar to <a href="https://sebastianraschka.com/blog/2020/numpy-intro.html">NumPy</a> arrays but have several additional features important for deep learning, e.g., automatic differentiation engine and GPU support.<br> * PyTorch's autograd system provides functions to compute gradients in dynamic computational graphs automatically.<br> A computation graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network.<br> * If we carry out computations in PyTorch, it will build a graph internally by default if one of its terminal nodes has the **requires_grad** attribute set to True.<br> * Computing the loss gradients in a computation graph involves applying the **chain rule** from right to left, called backpropagation. <br> * The gradients are used to update each parameter in a way that minimizes the loss function, such a method called **gradient descent**. <br> * By default, PyTorch destroys the computation graph after calculating the gradients to free memory. If we are going to reuse this computation graph, we can set **retain_graph=True** so that it stays in memory.<br> * PyTorch provides a high-level tools to automate backpropagation via calling **.backward** on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph.|
| Appendix A| Implementing multilayer neural networks| * When implementing a neural network in PyTorch, we typically subclass the **torch.nn.Module** class which allows us to encapsulate layers and operations and keep track of the model's parameters.<br> * Within this subclass, we define the network layers in the **__init\__** constructor and specify how they interact in the forward method.<br> * The outputs of the last layer of a neural network are called logits. <br> * Using **torch.nn.Sequential** can make our life easier if we have a series of layers that we want to execute in a specific order.<br><ul><li> after instantiating **self.layers = Sequential(...)** in the **__init\__** constructor, we just have to call the **self.layers** instead of calling each layer individually in the NeuralNetwork's forward method.</li></ul> * A linear layer multiplies the inputs with a weight matrix and adds a bias vector. This is sometimes referred to as a feedforward or fully connected layer.<br> * In deep learning, initializing model weights with small random numbers is desired to break symmetry during training -- otherwise, the nodes would be just performing the same operations and updates during backpropagation, which would not allow the network to learn complex mappings from inputs to outputs.<ul><li> we can make the random number initialization reproducible by seeding PyTorch's random number generator via torch.manual_seed(int).</li></ul> * When we use a model for inference (for instance, making predictions) rather than training, it is a best practice to use the **torch.no_grad()** context manager to tells PyTorch that it doesn't need to keep track of the gradients, which can result in significant savings in memory and computation.<br> * In PyTorch, it's common practice to code models such that they return the logits without passing them to a nonlinear activation function since PyTorch's commonly used loss functions combine the softmax operation with the negative log-likelihood loss in a single class for efficiency. |
| Appendix A | Data Loaders | * PyTorch implements a **Dataset** and a **DataLoader** class. The Dataset class is used to instantiate objects that define how each data record is loaded. The DataLoader handles how the data is shuffled and assembled into batches.<br> * In PyTorch, the three main components of a custom Dataset class are the __init\__ constructor, the __getitem\__ method, and the __len\__ method.<br> * Loading data without multiple workers (setting num_workers=0 ) will create a data loading bottleneck where the model sits idle until the next batch is loaded as illustrated in the left subpanel. If multiple workers are enabled, the data loader can already queue up the next batch in the background. <ul><li> if you are working with tiny datasets or interactive environments such as Jupyter notebooks, increasing num_workers may not provide any noticeable speedup. They might, in fact, lead to some issues.</li><li> One potential issue is the overhead of spinning up multiple worker processes, which could take longer than the actual data loading when your dataset is small.</li></ul> * In practice, we often use a third dataset, a so-called validation dataset, to find the optimal hyperparameter settings.<br> * model.train() and model.eval() are used to put the model into a training and an evaluation mode. This is necessary for components that behave differently during training and inference, such as dropout or batch normalization layers.<br> * It is important to include an optimizer.zero_grad() call in each update round to reset the gradients to zero. Otherwise, the gradients will accumulate, which may be undesired. | 