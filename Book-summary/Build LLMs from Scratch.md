| Chapter | Topic     | Notes      |
|---------|-----------|------------|
| 1       | What are Large Language Models?   |  * Large language models (LLMs) are deep neural network models for Natural Language Processing (NLP).<br>  * when we say language models "understand," we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.<br> * **Distinctions** between LLMs and earlier NLP models: <ul><li> LLMs are trained on vast quantities of text data. This allows LLMs to capture deeper contextual information and subtleties of human language.</li><li> NLP models typically designed for specific tasks, while LMs demonstrate a broader proficiency across a wide range of NLP tasks, e.g.,  ext translation, sentiment analysis, question answering.</li></ul> *  The success behind LLMs can be attributed to the <a href="https://arxiv.org/abs/1706.03762">transformer architecture</a> underpins many LLMs, and the **vast amounts of data** LLMs are trained on.<br> * The "large" in large language model refers to both the model's size in terms of parameters and the immense dataset on which it's trained.<br> *  LLM models often have tens or even hundreds of billions of parameters, the adjustable weights in the network optimized during training to **predict the next word** in a sequence.<br> * Existing LLMs can be adapted and finetuned to outperform general LLMs as well, which <a href="https://arxiv.org/abs/2305.09617">teams from Google Research and Google DeepMind</a> showed in a medical context and a team at Bloomberg showed via a version of GPT pretrained on finance data from scratch <a href="https://arxiv.org/abs/2303.17564">(BloombergGPT)</a>.<br> * The process of creating an LLM includes **pretraining** and **finetuning**.<ul><li> Pretraining: LLM is trained on large, diverse **unlabeled text data** to develop a broad understanding of language. In this phase, LLMs leverage <i>self-supervised learning</i>, where the model generates its own labels from the input data.</li><li> Finetuning: pretrained LLM is then trained on a narrower **labeled dataset** more specific to particular tasks or domains. The two most common categories for finetuning LLMs are:<ul><li> **Instruction-finetuning**: the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text.</li><li> **classification finetuning**, the labeled dataset consists of texts and associated class labels, for example, emails associated with spam and non-spam labels.</li></ul></ul> |
| 1     |  Using LLMs for Different Tasks | * The transformer consists of two parts, an **encoder** that processes the input text and produces an embedding representation (a numerical representation that captures many different factors in different dimensions) of the text that the decoder can use to generate the translated text one word at a time.<br> * The encoder and decoder consist of many layers connected by a so-called **self-attention mechanism**.<ul><li> The self-attention mechanism allows the model to weigh the importance of different words or tokens in a sequence relative to each other.</li><li> This mechanism enables the model to capture longrange dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.</li></ul> * BERT (Bidirectional Encoder Representation from Transformers) and GPT (Generative Pretrained Transformers) are later varients of the transformer architecture.<br> * BERT and its variants specialize in masked word prediction, which makes them suitable for text classification tasks (e.g., document classification and sentiment prediction).<br> * GPT models rely on the decoder submodule to perform text completion tasks, e.g., text summarization and code generation. GPT models work with eithr zero-shot learning or few-shot learning.<ul><li> Zero-shot learning: the ability to generalize to completely unseen tasks without any prior specific examples.</li><li> Few-shot learning: learning from a minimal number of examples the user provides as input.</li></ul> * Not all GPT models are transformers (some GPTs reply on recurrent convolutional networks, to improve computational efficiency) and not all transformers are GPT models (since transformers can also be used for computer vision).<br> * Pretraining LLMs requires access to significant resources and is very expensive. For example, GPT-3 pretraining cost is estimated to be $4.6 million in terms of cloud computing credits.<br> |
|  1   | A closer look at the GPT architecture  | * ChatGPT model was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI's InstructGPT paper: <a href="http://cdn.openai.com/researchcovers/language-unsupervised/language_understanding_paper.pdf">Finetuning with Human Feedback To Follow Instructions</a>.<br> * Self-supervised learning uses the next word in a sentence as the label that the model is supposed to predict.<br> * Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of **autoregressive** model.<br> * GPT-3 is significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total. |
| Appendix A  |  Introduction to PyTorch  | * <a href="https://pytorch. org/">Pytorch</a> is an open-source Python-based deep learning library. Despite its accessibility, it does not compromise on flexibility, providing advanced users the ability to tweak lower-level aspects of their models for customization and optimization.<br> * PyTorch's three main components include:<ul><li> A tensor library as a fundamental building block for efficient computing, which extends Numpy to offer a seamless switch between CPUs and GPUs; </li><li> automatic differentiation engine (aka autograd) which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization; and </li><li> deep learning library, that offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models.</li></ul> * If you do not have access to a GPU, there are several cloud computing providers where users can run GPU computations against an hourly cost. A popular Jupyter-notebook-like environment is [Google Colab](https://colab.research.google.com), [Kaggle Code](https://www.kaggle.com/code), [Paperspace Gradient](https://www.paperspace.com/gradient/free-gpu) which provides timelimited access to GPUs. |
| Appendix A | Understanding Tensors | * Tensors represent a mathematical concept that generalizes scalar values (0D), vectors (1D) and matrices (2D) to potentially higher dimensions.<br> * Tensors serve as data containers holding multi-dimensional data, where PyTorch can create, manipulate, and compute with these multidimensional arrays efficiently.<br> * PyTorch tensors are similar to <a href="https://sebastianraschka.com/blog/2020/numpy-intro.html">NumPy</a> arrays but have several additional features important for deep learning, e.g., automatic differentiation engine and GPU support.<br> * PyTorch's autograd system provides functions to compute gradients in dynamic computational graphs automatically.<br> A computation graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network.<br> * If we carry out computations in PyTorch, it will build a graph internally by default if one of its terminal nodes has the **requires_grad** attribute set to True.<br> * Computing the loss gradients in a computation graph involves applying the **chain rule** from right to left, called backpropagation. <br> * The gradients are used to update each parameter in a way that minimizes the loss function, such a method called **gradient descent**. <br> * By default, PyTorch destroys the computation graph after calculating the gradients to free memory. If we are going to reuse this computation graph, we can set **retain_graph=True** so that it stays in memory.<br> * PyTorch provides a high-level tools to automate backpropagation via calling **.backward** on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph.<br> *  | 