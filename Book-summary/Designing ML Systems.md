| Chapter | Topic     | Notes      | Additional Resources     |
|---------|-----------|------------|--------------------------|
| 1       | Summary   | • **ML**: Learn complex patterns from existing data and use these patterns to make predictions on unseen data<br>• **Zero-shot and continual learning**: Two approaches to learn with small data or no data<br>• **ML shines if**: The problem is repetitive, the cost of a wrong prediction is cheap, it's at scale, patterns are constantly changing<br>• **Key differences**: Between ML in research and in production<br>• **ML Systems versus Traditional Software**: Highlighting the distinct characteristics of ML systems compared to standard software |                                                              |
| 2       | Summary   | • **Characteristics of ML systems**: Most should have reliability, scalability, maintainability, adaptability<br>• **Decoupling objectives**: It's good practice to simplify the model development & maintenance as mentioned on pages 41-43                                    |                              |
| 3       | Summary   | • **First-party data**: Data your company collects about your customers.<br>• **Second-party data**: Data collected by another company about their own customers and made available to you.<br>• **Third-party data**: Companies collect data on the public who are not their direct customers.<br>• **Data formats**: JSON, row-Major (CSV), column-major (Parquet).<br>• **Data Models**: Relational data, NoSQL (document model, e.g., JSON, XML, BSON, and graph model).<br>• **Normalization in databases**: A good practice to reduce redundancy and improve integration.<br>• **Languages**: SQL is declarative; Python is imperative.<br>• **Query optimizer**: Examines all possible ways to execute a query and selects the fastest one.<br>• **Declarative ML**: User declares the features' schema and the task, the system then finds the best model.<br>• **Structured vs. Unstructured data**: Structured data follows predefined schema; unstructured data does not but may contain intrinsic patterns.<br>• **OLTP vs. OLAP**: Online transactional processing involves users; online analytical processing aggregates data in columns.<br>• **ETL**: Process to extract, transform, and load data into the target destination.<br>• **Modes of data flow**: Through databases, services, or real-time transport (e.g., Kafka). |                                    |
| 4       | Data Augmentation (DA)               | • Types of DA: label-preserving transformation, perturbation (adding random noise), data synthesis<br>• In NLP, templates is a cheap way to synthesis training data<br>• In computer vision, the mixup method synthesis data via combining existing examples           |       |
| 4       | Class imbalance                      | • Class imbalance: caused by biases in the sampling process or labeling errors<br>• Sensitivity to class imbalance increases with the complexity of the predictive task<br>    - Linearly-separable problems are unaffected by all levels of class imbalance<br>    - Deep neural networks performed much better on imbalanced data than shallower neural networks<br>• Handling class imbalance: choosing the right metrics, data-level methods, algorithmic-level methods<br>• Data-level methods: <br>    - undersampling (Tomek Links; risk of losing important information) or <br>    - oversampling (SMOTE; risk of overfitting)<br>    - Advanced methods: Two-phase learning, dynamic sampling<br>• Algorithmic methods:<br>    - Cost-sensitive learning: modify loss function to consider the varying costs of majority and minority classes<br>    - Class-balanced loss: assign weight of each class proportional to the number of samples in that class<br>    - Focal loss: higher weight assigned to samples with low probability of being right |       |
| 4       | Labeling                             | • Hand labels are expensive, raise privacy concerns, slow, and difficult to update<br>• Label multiplicity (disagreement among annotators), data lineage (keep track of origin of data sample & labels)<br>• Natural labels (ground truth labels): labels from the user feedback<br>• Feedback loop length: time it takes from when a prediction is served until when the feedback on it is received.<br>• Handling lack of labels: Weak-supervision, semi-supervision, transfer learning, active learning, data augmentation<br>• Self-training: use predictions on small labeled data as new labels to enhance the training data<br>• Perturbation-based method (data augmentation): small perturbations to a sample shouldn't change its label<br>• Active learning: label the samples most helpful to the model selected based on some metrics or heuristics<br>    - Such as uncertainty measurement, query by committee (disagreement among multiple candidate models)<br>    - Compare between Active learning and Data Valuation for active data acquisition |       |
| 4       | Sampling                             | • Understanding sampling methods (non-probability, random sampling) --> avoid potential sampling biases<br>• Non-probability sampling: convenience, snowball, judgement, quota sampling (biases typically occur)<br>• Probability sampling: simple random sampling, stratified sampling, weighted sampling, reservoir sampling (streaming data), importance sampling       |       |
| 5       | Common Feature Engineering Operations | • missing values: Missing at random (MAR), missing completely at random (MCAR), missing not at random (MNAR)<br>    - MNAR: reason is because of the true value itself (e.g., respondents do not like to disclose their income)<br>    - MAR: reason is not related to the value itself, but to another observed variable (e.g., females do not like to disclose their age)<br>    - MCAR: no pattern in when the value is missing (rare cases)<br>• Deletion: cause loss of information, create biases (especially with MAR)<br>• Imputation: you risk injecting your own bias and adding noise, or worse, data leakage.<br>• Standard scaling assumes that the data follows normal distribution<br>• Feature scaling: if the features follow a skewed distribution, use [log transformation](https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9) to mitigate the skewness (this does not work well for all cases)<br>• Scaling requires global statistics, hence it can be a source of data leakage during inference; or the statistics will be less accurate if the new data during inference deviates from the original data<br>• [Hashing trick](https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f): solve the problem of having categorical features with changing categories (collisionmay occur)<br>• Feature crossing: combining two or more features to generate new features to model nonlinear relationships<br>• Embedding: vector represents a piece of data. Word embeddings are the most common use of embeddings<br>• Causes of data leakage: splitting time-correlated data randomly instead of by time, scaling before splitting, filling in missing data with statistics from the test split, poor handling of data duplication before splitting, group leakage, leakage from data generation process<br>• Addressing data leakage: ablation study, measuring predictive power of each feature, monitoring new features added<br>• More features does not mean better model performance, reasons: more opportunities for data leakage, may cause overfitting, increase memory required, increase inference latency, technical debts (updating data becomes a cumbersome task) |        |
|  6       | Summary | • Four phases on ML model development: <ol><li>before ML (try solving the problem without ML), </li><li>simplest ML models, </li><li>optimizing simplest models, </li><li>complex models</li></ol> • Classical ML models are still used in various applications, e.g., recommender systems & classification tasks with strict latency requirements.<br> • Six tips for model selection: avoid the state-of-the-art trap, start with the simplest model, avoid human biases in selecting models, evaluate good performance now versus good performance later, evaluate trade-offs (false positive vs. false negatives, compute requirement vs. accuracy, etc.), understand model's assumptions.<br> • Ensembles are less favored in production because they are more complex to deploy and harder to maintain. However, they are being used when a small performance boost leads to huge financial gain.<br> • There are three ways to create ensembles: bagging (bootstrap aggregation, Random forest), boosting, & stacking <ul><li> Bagging: sample with replacement to create different datasets, called bootstraps, and train a model on each of these bootstraps. The final prediction is obtained by majority vote (classification) or the average (regression) </li><li>Bagging improves unstable models, such as neural networks, and decision trees. </li><li> Boosting: convert weak learners to strong one. Each learner is trained on the same set of samples, but the samples are weighted differently among iterations. Hence, future learners focus more on the samples that previous weak learners misclassified. </li><li> Stacking: training base learners on the training data, then create a meta-learner that combines the output of base learners. The meta-learner can be majority vote or average vote.</li></ul> • Experiment tracking: the process of tracking the progress and results of an experiment.<br>	• Tools such as MLflow and Weights & Biases<br> • Versioning: logging all the details of an experiment to reproduce or compare the results with other experiments.<br> • Tools such as DVC and Deltalake<br> • Data versioning is challenging: (1) data size is often large, (2) dataset might not fit into a local machine, (3) there is a confusion of what a Diff is, (4) not clear how to resolve merge conflicts, (5) regulations, GDPR, makes it complicated to version users data<br> • Debugging ML models: (1) ML models fail silently, (2) it may become slow to validate whether the bug has been fixed, (3) debugging ML models is hard due to their cross-functional complexity.<br>  • What makes  a ML model to fail: (1) Theoretical constraints, (2) poor implementation of the model, (3) poor choice of hyperparameters, (4) data problems, (5) poor choice of features<br> • [Best practices for training and debugging ML models](http://karpathy.github.io/2019/04/25/recipe/): (start simple and gradually add more components), (2) overfit a single batch, (3) set a random seed<br> • Distributed training: Gradient checkpointing (approach to train using less memory footprint), data parallelism (distribute data and aggregate weights), model parallelism (distribute model layers)<br> • Soft autoML: hyperparameter tuning<br> • Hard autoML: architecture search and learned optimizer<br> • Evaluation metrics, by themselves, mean little. When evaluating your model, it's essential to know the baseline you're evaluating it against. These baselines can be either: (1) random baseline, (2) simple heuristic, (3) zero rule baseline (always predict the most common class), (4) Human baseline, or (5) existing solutions<br> • Evaluation methods:<br> • perturbation tests: test model on noisy input rather than clean data<br> • invariance tests: detect biases via altering or removing sensitive information from the features<br> • directional expectation tests: if the output changes in the opposite expected direction --> model not learning<br>  • model calibration: Platt scaling method used to calibrate your model<br> • confidence measurement: measuring confidence about each prediction (certainty threshold)<br> • slice-based evaluation: useful to detect Simpson's paradox |          |
|  7       | Summary | • The hard parts of deploying ML models: <ul><li>Make them available to millions of users with a latency of milliseconds and 99% uptime</li><li>Setting up infrastructure to notify the right person when something goes wrong</li><li>Being able to figure out what went wrong</li><li>Seamlessly deploying updates to fix what's wrong</li></ul> • Serialization: converting ML model into a format that can be used by other applications. <ul><li>Two parts of ML model are exported together: model architecture and model's parameter values</li></ul> • Online prediction (on-demand) <ul><li>Generating predictions as soon as requests for these predictions arrive</li><li>It uses either batch features or combination of streaming and batch features</li></ul> • Batch prediction <ul><li>Predictions are generated periodically or whenever triggered</li><li>Predictions are stored in SQL tables or in-memory database, and retrieved as needed</li></ul> • Hybrid approach is to precompute predictions for popular queries, then generate predictions online for less popular queries. • Model compression: low-rank factorization, knowledge distillation, pruning, and quantization. <ul><li>Low rank factorization: replacing high-dimensional tensors with lower-dimensional tensors</li><li>Knowledge distillation: train a student (small) model to mimic a larger model (teacher)</li><li>Pruning: remove redundant and uncritical nodes from a neural network or finding parameters least useful to prediction and set them to zero</li><li>Quantization: use fewer bits to represent the model parameters (most common method)</li></ul> • Pros of edge deployment: reasonable cost, no rely on Internet connection, no worry about network latency, appealing when handling sensitive data • Instead of targeting new compilers and libraries for every new hardware backend, it makes sense to create a middleman to bridge frameworks, e.g., PyTorch and TensorFlow, and platforms, e.g., CPUs, GPUs, and TPUs. • Lowering: a process in which the compiler generates a series of high- and low-level intermediate representations before generating the code native to a hardware backend. • Even if ML models work fine in development, they can be slow when deployed in production • Models can be optimized locally or globally <ul><li>Local: optimizing an operator or a set of operators of the model (reducing memory access or parallelization)</li><li>Global: optimizing the entire computation graph end-to-end</li></ul> • AutoTVM is a ML-based compiler stack used to automatically optimize ML models • WebAssembly is an open standard that allows you to run executable programs in browsers <ul><li>After building ML models, we can compile them to WebAssembly to get an executable file used with JavaScript (it is slower than running models natively on devices, e.g., Android or iOS apps).</li></ul> |           |
| 11      | Infrastructure abstraction   | • It is difficult for data scientists to equally focus on model development and on model deployment. <br>• However, there are several drawbacks for splitting the team into ML engineers and DevOps engineers:<br>    - Communication and coordination overhead<br>    - Debugging challenges: when trying to figure out where the failures come from<br>    - Finger-pointing: each team might think it's another team's responsibility to fix the failures<br>    - Narrow context: no one has visibility into the entire process to optimize it<br>• A solution to this dilemma is to allow data scientists to own the entire process end-to-end without having to worry about the infrastructure:<br>    - What if I can tell the tool, "Here where I store my data (S3), here are the steps to run my code (featurization, modeling), here's where my code should run (EC2 instances, serverless stuff like AWS Batch, Functions, etc.), here's what my code needs to run at each step (dependencies)," and then this tool manages all the infrastructure stuff for the data scientist.<br>• Can we use large language models to generate configuration files for each infrastructure generated by such a tool? | • MLOps templates, e.g., <br>[Cookiecutter template for MLOps](https://github.com/mlops-guide/mlops-template)<br>• [MLOps Guide](https://mlops-guide.github.io/)<br>• [DagsHub / Cookiecutter-MLOps](https://dagshub.com/DagsHub/Cookiecutter-MLOps)<br>    - A reasonably standardized but flexible project structure that implements sound MLOps principles for building your next machine learning project <br>    - It includes only DVC<br>• [mlops-generator 1.0.1](https://pypi.org/project/mlops-generator/) <br>    - CLI for MLOps code generator |