# ML blogs & Articles 

This page contains plenty of blogs and articles on various ML topics, including:

* [Large Language Models](#large-language-models)
* [Traditional ML and Deep Learning](#traditional-ml--dl)


## Large Language Models

* [Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/): interesting visual storytelling of how LLMs and transformers work.

* [AutoGen: Enabling next-generation large language model applications](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/): AutoGen is a framework for simplifying the orchestration, optimization, and automation of LLM workflows.

* [Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/#guardrails-to-ensure-output-quality): There are seven key patterns. They’re also organized along the spectrum of improving performance vs. reducing cost/risk, and closer to the data vs. closer to the user.

* [Gorilla AI Creator Unveils How It Equips LLMs With Real-World Skills and Knowledge](https://medium.com/@raphael.mansuy/gorilla-ai-creator-unveils-how-it-equips-llms-with-real-world-skills-and-knowledge-6b36de5040db): Gorilla AI founder Shishir Patil explored the method of imparting external knowledge and skills, such as actions and API calls, to large language models (LLMs) to enhance their utility and safety for deployment in real-world scenarios ([Talk](https://www.youtube.com/watch?v=9iSxF_RW6xk&t=22s)).

* [Llama from scratch](https://blog.briankitano.com/llama-from-scratch/): This article offers a step-by-step guide on implementing the Llama starting small using the TinyShakespeare dataset. The approach in this tutorial is iterative, starting with basic models and gradually integrating Llama's unique features like RMSNorm, Rotary embeddings, and the SwiGLU activation function. 

* [What are embeddings? A Book Review](https://medium.com/@raphael.mansuy/embeddings-the-foundation-of-modern-ai-1512f7ecbd6): What are Embeddings? is a comprehensive book written by Vicky Boykis under a Creative Commons license. The book provides a deep dive into the concept of embeddings.

* [Anti-hype LLM reading list](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e): This is actually a really good list of papers and reading materials on LLMs.

* [The Generative AI Revolution: Exploring the Current Landscape](https://pub.towardsai.net/the-generative-ai-revolution-exploring-the-current-landscape-4b89998fcc5f): This post explains the journey of how it all started, where it is going, and some of the biggest players and most popular models in the Generative AI landscape today, along with real-world tools designed for users to optimize creation, ideation, development, and production processes.

* [Do Foundation Model Providers Comply with the Draft EU AI Act?](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html): In this article, the authors evaluate whether major foundation model providers currently comply with the draft requirements of the EU AI Act and find that they largely do not. 

* [The complete guide to LLM fine-tuning](https://bdtechtalks.com/2023/07/10/llm-fine-tuning/): This article is part of Demystifying AI, a series of posts that (try to) disambiguate the jargon and myths surrounding AI.

* [LangChain: How to Build ChatGPT for Your Data](https://www.youtube.com/watch?v=Azfc-TjG9Tg): you will learn all about the key technologies that enable Generative AI practitioners like you to leverage LangChain for building complex LLM applications.  

* [Large Language Models and the Future of Custom, Fine-tuned LLMs](https://outerbounds.com/blog/custom-llm-tuning/): A great article from Outerbounds, the company behind Metaflow, which explores instruction tuning for large language models. The authors discuss the rise of open-source LLMs, the role of commercial entities in the field, and the challenges of applying LLMs, including hardware access, real-world application, and ethical issues.

* [Introducing no-code LLM FineTuning with Monster API](https://blog.monsterapi.ai/no-code-fine-tuning-llm/): the article introduces a new no-code LLM FineTuning product, designed to simplify and expedite the fine-tuning process while providing you with all the capabilities and possibilities you need.

* [Understanding transformers from first principles](https://www.linkedin.com/pulse/understanding-transformers-from-first-principles-115-ajit-jaokar%3FtrackingId=52FlIkyVR6mEaV88UDnxNQ%253D%253D/?trackingId=52FlIkyVR6mEaV88UDnxNQ%3D%3D): In this blog post, the author explains the concept of transformers from first principles.

* [Few go-to resources to Understand LLMs / GenAIs ?](https://www.linkedin.com/posts/prabakaranchandrantheds_ai-python-computervision-activity-7081346854528954368-i4B1/?utm_source=share&utm_medium=member_ios): Four learning resources to learn the fundamentals of Deep learning and to understand the basics of LLMs/GenAIs.

* [From Novice to Expert: A Comprehensive Step-by-Step Study Plan for Mastering LLMs](https://pub.towardsai.net/from-novice-to-expert-a-comprehensive-step-by-step-study-plan-for-mastering-llms-dc9feb60ecc4): this article presents a step-by-step study plan to guide you from a novice to an expert in LLMs.

* [transformers](https://github.com/0xsanny/transformers): A collection of resources to study Transformers in depth. 

* [LLMS_Library_2023](https://github.com/rashmimarganiatgithub/LLMS_Library_2023): a comprehensive repository serves as a one-stop resource hands-on code, insightful summaries, and other valuable resources. 

* [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM): a curated list of LLMs resources.

* [Prompt Engineering Guide](https://www.promptingguide.ai/):  A prompt engineering guide that contains all the latest papers, learning guides, models, lectures, references, new LLM capabilities, and tools related to prompt engineering.

* [dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide): Motivated by the high interest in developing with LLMs, the authors have created this new prompt engineering guide that contains all the latest papers, learning guides, lectures, references, and tools related to prompt engineering for LLMs.

* [Learn Prompting: Prompt Engineering Guide](https://learnprompting.org/docs/intro): The authors have created a comprehensive guide on how to use Generative AI. This course is tailored to non-technical readers, who may not have even heard of AI, making it the perfect starting point if you are new to Generative AI and Prompt Engineering. Technical readers will find valuable insights within the later modules.

* [Large Language Model Cheatsheet](https://docs.google.com/presentation/d/1ytGfwc8tIKolDQ_jMOmhWVC8t41klYa5/edit?usp=share_link&ouid=100484802817147345492&rtpof=true&sd=true): three slides summairzing the main ideas and concepts of large language models.

* [The Practical Guides for Large Language Models](https://github.com/Mooler0410/LLMsPracticalGuide): A curated (still actively updated) list of practical guide resources of LLMs. It's based on our survey paper.

* [How to train your own Large Language Models?](https://blog.replit.com/llm-training): The blog post explains how to train your own custom large language models, using MosaicML, Databricks, and Hugging Face. 

* [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html): This awesome post consists of three parts:
	- Part 1 discusses the key challenges of productionizing LLM applications and the solutions that I’ve seen.
    - Part 2 discusses how to compose multiple tasks with control flows (e.g. if statement, for loop) and incorporate tools (e.g. SQL executor, bash, web browsers, third-party APIs) for more complex and powerful applications.
	- Part 3 covers some of the promising use cases that I’ve seen companies building on top of LLMs and how to construct them from smaller tasks.
	
* [Understanding Large Language Models -- A Transformative Reading List by Sebastian Raschka](https://sebastianraschka.com/blog/2023/llm-reading-list.html): The author collected important research papers to understand the large language models. The reading list will help you understand the breakthroughs in the NLP space over the years: from RNNs in the pre-transformer era to Google BERT to today’s ChatGPT.

* [Jay Alammar’s Article Series on Large Language Models](https://medium.com/geekculture/top-resoruces-to-learn-understand-large-language-models-4d339f7b685d): Jay has written multiple blogs on transformers and large language models that are really impressive and very unique to understanding the basics of large language models.

* [Transformer models: an introduction and catalog — 2023 Edition](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/): A Catalog of Transformer AI models.

* [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/): An in-depth explanation of how ChatGPT works and what really lets ChatGPT work?

* [𝗛𝘂𝗻𝗱𝗿𝗲𝗱𝘀 𝗼𝗳 𝗡𝗟𝗣 𝗣𝗮𝗽𝗲𝗿𝘀 𝘄𝗶𝘁𝗵 𝗖𝗼𝗱𝗲](https://index.quantumstat.com/): The NLP Index lets you keep track of the NLP advancements, by offering an updated list of the latest NLP research papers! Furthermore, the website provides a link for the Github repository of each paper, making it a valuable resource for ML researchers and practitioners.

* [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223): In this survey, the authors review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, they focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.


## Traditional ML & DL

* [ML Papers of The Week](https://github.com/dair-ai/ML-Papers-of-the-Week): The repo highlights some of the top trending ML papers every week.  

* [ML-Papers-Explained](https://github.com/dair-ai/ML-Papers-Explained): Explanations to key concepts in ML.

* [Machine Learning Mastery](https://machinelearningmastery.com/start-here/):  Step-by-step guides for getting started with applied ML. 

* [Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI](https://arxiv.org/abs/2201.00650): This reference is a home to hundreds of fully-solved problems, from a wide range of key topics in AI.

* [From Zero to AI Research Scientist Full Resources Guide](https://github.com/ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide): A guide for anyone with basic programming knowledge or a computer science background interested in becoming a Research Scientist with 🎯 on Deep Learning and NLP.

* [Awesome AI Guidelines](https://github.com/EthicalML/awesome-artificial-intelligence-guidelines): This repository maps the ecosystem of guidelines, principles, codes of ethics, standards and regulation being put in place around artificial intelligence.

* [Papers With Code: The latest in Machine Learning](https://paperswithcode.com/): A list of machine learning papers, their datasets, and their source code.

* [labml.ai Annotated PyTorch Paper Implementations](https://nn.labml.ai/): A collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations, and the website renders these as side-by-side formatted notes.

* [Your Guide to Data Quality Management](https://www.scnsoft.com/blog/guide-to-data-quality-management): tips on how a company can measure and improve the quality of their data.

* [Deep Learning Tuning Playbook, Google & Harvard](https://github.com/google-research/tuning_playbook): If you have ever wondered about the process of hyperparameter tuning, this guide is for you. 

* [FloydHub Blog](https://blog.floydhub.com/): A list of articles about all topics in AI, books recommendation, datasets, etc.

* [Awesome-conformal-prediction](https://github.com/valeman/awesome-conformal-prediction): The most comprehensive professionally curated resource on Conformal Prediction including the best tutorials, videos, books, papers, articles, courses, websites, conferences and open-source libraries code in Python, R and Julia.

* [The Base Camp for your Ascent in Machine Learning](https://databasecamp.de/en/homepage): In this blog, you will find all the topics to help you master the expedition into the world of Artificial Intelligence. 

* [CS 229 ― Machine Learning](https://stanford.edu/~shervine/teaching/cs-229/): A set of illustrated ML cheatsheets covering the content of the CS 229 class at Stanford.

